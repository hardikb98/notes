{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddd43d5c",
   "metadata": {},
   "source": [
    "#### 1. What do you mean by Data Analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4bcdf8",
   "metadata": {},
   "source": [
    "Data analysis is a multidisciplinary field of data science, in which data is analyzed using mathematical, statistical, and computer science with domain expertise to discover useful information or patterns from the data. It involves gathering, cleaning, transforming, and organizing data to draw conclusions, forecast, and make informed decisions. The purpose of data analysis is to turn raw data into actionable knowledge that may be used to guide decisions, solve issues, or reveal hidden trends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cf772f",
   "metadata": {},
   "source": [
    "#### 2. What is Data Wrangling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9743b83d",
   "metadata": {},
   "source": [
    "Data Wrangling is very much related concepts to Data Preprocessing. It's also known as Data munging. It involves the process of cleaning, transforming, and organizing the raw, messy or unstructured data into a usable format. The main goal of data wrangling is to improve the quality and structure of the dataset. So, that it can be used for analysis, model building, and other data-driven tasks.\n",
    "\n",
    "Data wrangling can be a complicated and time-consuming process, but it is critical for businesses that want to make data-driven choices. Businesses can obtain significant insights about their products, services, and bottom line by taking the effort to wrangle their data.\n",
    "\n",
    "Some of the most common tasks involved in data wrangling are as follows:\n",
    "\n",
    "- Data Cleaning: Identify and remove the errors, inconsistencies, and missing values from the dataset.\n",
    "- Data Transformation: Transformed the structure, format, or values of data as per the requirements of the analysis. that may include scaling & normalization, encoding categorical values.\n",
    "- Data Integration: Combined two or more datasets, if that is scattered from multiple sources, and need of consolidated analysis.\n",
    "- Data Restructuring: Reorganize the data to make it more suitable for analysis. In this case, data are reshaped to different formats or new variables are created by aggregating the features at different levels.\n",
    "- Data Enrichment: Data are enriched by adding additional relevant information, this may be external data or combined aggregation of two or more features.\n",
    "- Quality Assurance: In this case, we ensure that the data meets certain quality standards and is fit for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34f810a",
   "metadata": {},
   "source": [
    "#### 3. What is the difference between descriptive and predictive analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be796018",
   "metadata": {},
   "source": [
    "Descriptive and predictive analysis are the two different ways to analyze the data.\n",
    "\n",
    "Descriptive Analysis: Descriptive analysis is used to describe questions like \"What has happened in the past?\" and \"What are the key characteristics of the data?\". Its main goal is to identify the patterns, trends, and relationships within the data. It uses statistical measures, visualizations, and exploratory data analysis techniques to gain insight into the dataset.\n",
    "The key characteristics of descriptive analysis are as follows:\n",
    " - Historical Perspective: Descriptive analysis is concerned with understanding past data and events.\n",
    " - Summary Statistics: It often involves calculating basic statistical measures like mean, median, mode, standard deviation, and percentiles.\n",
    " - Visualizations: Graphs, charts, histograms, and other visual representations are used to illustrate data patterns.\n",
    " - Patterns and Trends: Descriptive analysis helps identify recurring patterns and trends within the data.\n",
    " - Exploration: It's used for initial data exploration and hypothesis generation.\n",
    "\n",
    "Predictive Analysis: Predictive Analysis, on the other hand, uses past data and applies statistical and machine learning models to identify patterns and relationships and make predictions about future events. Its primary goal is to predict or forecast what is likely to happen in future.\n",
    "The key characteristics of predictive analysis are as follows:\n",
    " - Future Projection: Predictive analysis is used to forecast and predict future events.\n",
    " - Model Building: It involves developing and training models using historical data to predict outcomes.\n",
    " - Validation and Testing: Predictive models are validated and tested using unseen data to assess their accuracy.\n",
    " - Feature Selection: Identifying relevant features (variables) that influence the predicted outcome is crucial.\n",
    " - Decision Making: Predictive analysis supports decision-making by providing insights into potential outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3853565",
   "metadata": {},
   "source": [
    "#### 4. What is univariate, bivariate, and multivariate analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbc5a02",
   "metadata": {},
   "source": [
    "Univariate, Bivariate and multivariate are the three different levels of data analysis that are used to understand the data.\n",
    "\n",
    "- Univariate analysis: Univariate analysis analyzes one variable at a time. Its main purpose is to understand the distribution, measures of central tendency (mean, median, and mode), measures of dispersion (range, variance, and standard deviation), and graphical methods such as histograms and box plots. It does not deal with the courses or relationships from the other variables of the dataset. \n",
    "Common techniques used in univariate analysis include histograms, bar charts, pie charts, box plots, and summary statistics.\n",
    "\n",
    "- Bivariate analysis: Bivariate analysis involves the analysis of the relationship between the two variables. Its primary goal is to understand how one variable is related to the other variables. It reveals, Are there any correlations between the two variables, if yes then how strong the correlations is? It can also be used to predict the value of one variable from the value of another variable based on the found relationship between the two.\n",
    "Common techniques used in bivariate analysis include scatter plots, correlation analysis, contingency tables, and cross-tabulations.\n",
    "\n",
    "- Multivariate analysis: Multivariate analysis is used to analyze the relationship between three or more variables simultaneously. Its primary goal is to understand the relationship among the multiple variables. It is used to identify the patterns, clusters, and dependencies among the several variables.\n",
    "\n",
    "Common techniques used in multivariate analysis include principal component analysis (PCA), factor analysis, cluster analysis, and regression analysis involving multiple predictor variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440cdc1a",
   "metadata": {},
   "source": [
    "#### 5. What are the steps you would take to analyze a dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d408fae",
   "metadata": {},
   "source": [
    "Data analysis involves a series of steps that transform raw data into relevant insights, conclusions, and actionable suggestions. While the specific approach will vary based on the context and aims of the study, here is an approximate outline of the processes commonly followed in data analysis:\n",
    "\n",
    "- Problem Definition or Objective: Make sure that the problem or question you're attempting to answer is stated clearly. Understand the analysis's aims and objectives to direct your strategy.\n",
    "- Data Collection: Collate relevant data from various sources. This might include surveys, tests, databases, web scraping,  and other techniques. Make sure the data is representative and accurate.ALso\n",
    "- Data Preprocessing or Data Cleaning: Raw data often has errors, missing values, and inconsistencies. In Data Preprocessing and Cleaning, we redefine the column's names or values, standardize the formats, and deal with the missing values.\n",
    "- Exploratory Data Analysis (EDA): EDA is a crucial step in Data analysis. In EDA, we apply various graphical and statistical approaches to systematically analyze and summarize the main characteristics, patterns, and relationships within a dataset. The primary objective behind the EDA is to get a better knowledge of the data's structure, identify probable abnormalities or outliers, and offer initial insights that can guide further analysis.\n",
    "- Data Visualizations: Data visualizations play a very important role in data analysis. It provides visual representation of complicated information and patterns in the data which enhances the understanding of data and helps in identifying the trends or patterns within a data. It enables effective communication of insights to various stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0118cd7",
   "metadata": {},
   "source": [
    "#### 6. What is data cleaning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb81f06d",
   "metadata": {},
   "source": [
    "Data cleaning is the process of identifying the removing misleading or inaccurate records from the datasets. The primary objective of Data cleaning is to improve the quality of the data so that it can be used for analysis and predictive model-building tasks. It is the next process after the data collection and loading.\n",
    "\n",
    "In Data cleaning, we fix a range of issues that are as follows:\n",
    "\n",
    "- Inconsistencies: Sometimes data stored are inconsistent due to variations in formats, columns_name, data types, or values naming conventions. Which creates difficulties while aggregating and comparing. Before going for further analysis, we correct all these inconsistencies and formatting issues.\n",
    "- Duplicate entries: Duplicate records may biased analysis results, resulting in exaggerated counts or incorrect statistical summaries. So, we also remove it.\n",
    "- Missing Values: Some data points may be missing. Before going further either we remove the entire rows or columns or we fill the missing values with probable items.\n",
    "- Outlier: Outliers are data points that drastically differ from the average which may result in machine error when collecting the dataset. if it is not handled properly, it can bias results even though it can offer useful insights. So, we first detect the outlier and then remove it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906f7278",
   "metadata": {},
   "source": [
    "#### 7. What is the importance of exploratory data analysis (EDA) in data analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592dfa7d",
   "metadata": {},
   "source": [
    "Exploratory data analysis (EDA) is the process of investigating and understanding the data through graphical and statistical techniques. It is one of the crucial parts of data analysis that helps to identify the patterns and trends in the data as well as help in understanding the relationship between variables.\n",
    "\n",
    "EDA is a non-parametric approach in data analysis, which means it does take any assumptions about the dataset. EDA is important for a number of reasons that are as follows:\n",
    "\n",
    "- With EDA we can get a deep understanding of patterns, distributions, nature of data and relationship with another variable in the dataset.\n",
    "- With EDA we can analyze the quality of the dataset by making univariate analyses like the mean, median, mode, quartile range, distribution plot etc and identify the patterns and trends of single rows of the dataset.\n",
    "- With EDA we can also get the relationship between the two or more variables by making bivariate or multivariate analyses like regression, correlations, covariance, scatter plot, line plot etc.\n",
    "- With EDA we can find out the most influential feature of the dataset using correlations, covariance, and various bivariate or multivariate plotting.\n",
    "With EDA we can also identify the outliers using Box plots and remove them further using a statistical approach.\n",
    "- EDA provides the groundwork for the entire data analysis process. It enables analysts to make more informed judgments about data processing, hypothesis testing, modelling, and interpretation, resulting in more accurate and relevant insights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3547a21",
   "metadata": {},
   "source": [
    "#### 8. What is Time Series analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f20453",
   "metadata": {},
   "source": [
    "Time Series analysis is a statistical technique used to analyze and interpret data points collected at specific time intervals. Time series data is the data points recorded sequentially over time. The data points can be numerical, categorical, or both. The objective of time series analysis is to understand the underlying patterns, trends and behaviours in the data as well as to make forecasts about future values.\n",
    "\n",
    "The key components of Time Series analysis are as follows:\n",
    "\n",
    "- Trend: The data's long-term movement or direction over time. Trends can be upward, downward, or flat.\n",
    "- Seasonality: Patterns that repeat at regular intervals, such as daily, monthly, or yearly cycles.\n",
    "- Cyclical Patterns: Longer-term trends that are not as regular as seasonality, and are frequently associated with economic or business cycles.\n",
    "Irregular Fluctuations: Unpredictable and random data fluctuations that cannot be explained by trends, seasonality, or cycles.\n",
    "- Auto-correlations: The link between a data point and its prior values. It quantifies the degree of dependence between observations at different time points.\n",
    "\n",
    "Time series analysis approaches include a variety of techniques including Descriptive analysis to identify trends, patterns, and irregularities, smoothing techniques like moving averages or exponential smoothing to reduce noise and highlight underlying trends, Decompositions to separate the time series data into its individual components and forecasting technique like ARIMA, SARIMA, and Regression technique to predict the future values based on the trends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57204535",
   "metadata": {},
   "source": [
    "#### 9. What is Feature Engineering?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765626c9",
   "metadata": {},
   "source": [
    "Feature engineering is the process of selecting, transforming, and creating features from raw data in order to build more effective and accurate machine learning models. The primary goal of feature engineering is to identify the most relevant features or create the relevant features by combining two or more features using some mathematical operations from the raw data so that it can be effectively utilized for getting predictive analysis by machine learning model.\n",
    "\n",
    "The following are the key elements of feature engineering:\n",
    "\n",
    "- Feature Selection: In this case we identify the most relevant features from the dataset based on the correlation with the target variables.\n",
    "- Create new feature: In this case, we generate the new features by aggregating or transforming the existing features in such a way that it can be helpful to capture the patterns or trends which is not revealed by the original features.\n",
    "- Transformation: In this case, we modify or scale the features so, that it can helpful in building the machine learning model. Some of the common transformations method are Min-Max Scaling, Z-Score Normalization, and log transformations etc.\n",
    "- Feature encoding: Generally ML algorithms only process the numerical data, so, that we need to encode categorical features into the numerical vector. Some of the popular encoding technique are One-Hot-Encoding, Ordinal label encoding etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9fc4d7",
   "metadata": {},
   "source": [
    "#### 10. What's the difference between structured and unstructured data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2856dbb",
   "metadata": {},
   "source": [
    "Structured and unstructured data depend on the format in which the data is stored. Structured data is information that has been structured in a certain format, such as a table or spreadsheet. This facilitates searching, sorting, and analyzing. Unstructured data is information that is not arranged in a certain format. This makes searching, sorting, and analyzing more complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b83009",
   "metadata": {},
   "source": [
    "#### 11. What is the difference between pandas Series and pandas DataFrames?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9cbefa",
   "metadata": {},
   "source": [
    "In pandas, Both Series and Dataframes are the fundamental data structures for handling and analyzing tabular data. However, they have distinct characteristics and use cases.\n",
    "\n",
    "A series in pandas is a one-dimensional labelled array that can hold data of various types like integer, float, string etc. It is similar to a NumPy array, except it has an index that may be used to access the data. The index can be any type of object, such as a string, a number, or a datetime.\n",
    "\n",
    "A pandas DataFrame is a two-dimensional labelled data structure resembling a table or a spreadsheet. It consists of rows and columns, where each column can have a different data type. A DataFrame may be thought of as a collection of Series, where each column is a Series with the same index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7868c67",
   "metadata": {},
   "source": [
    "#### 12. What is the difference between descriptive and inferential statistics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptive statistics and inferential statistics are the two main branches of statistics\n",
    "\n",
    "- Descriptive Statistics: Descriptive statistics is the branch of statistics, which is used to summarize and describe the main characteristics of a dataset. It provides a clear and concise summary of the data's central tendency, variability, and distribution. Descriptive statistics help to understand the basic properties of data, identifying patterns and structure of the dataset without making any generalizations beyond the observed data. Descriptive statistics compute measures of central tendency and dispersion and also create graphical representations of data, such as histograms, bar charts, and pie charts to gain insight into a dataset.\n",
    "\n",
    "Descriptive statistics is used to answer the following questions:\n",
    "What is the mean salary of a data analyst?\n",
    "What is the range of income of data analysts?\n",
    "What is the distribution of monthly incomes of data analysts?\n",
    "\n",
    "- Inferential Statistics: Inferential statistics is the branch of statistics, that is used to conclude, make predictions, and generalize findings from a sample to a larger population. It makes inferences and hypotheses about the entire population based on the information gained from a representative sample. Inferential statistics use hypothesis testing, confidence intervals, and regression analysis to make inferences about a population.\n",
    " \n",
    "Inferential statistics is used to answer the following questions:\n",
    "Is there any difference in the monthly income of the Data analyst and the Data Scientist?\n",
    "Is there any relationship between income and education level?\n",
    "Can we predict someone's salary based on their experience?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. What are measures of central tendency?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91def8c",
   "metadata": {},
   "source": [
    "Measures of central tendency are the statistical measures that represent the centre of the data set. It reveals where the majority of the data points generally cluster. The three most common measures of central tendency are:\n",
    "\n",
    "- Mean: The mean, also known as the average, is calculated by adding up all the values in a dataset and then dividing by the total number of values. It is sensitive to outliers since a single extreme number can have a large impact on the mean.\n",
    "Mean = (Sum of all values) / (Total number of values)\n",
    "\n",
    "- Median: The median is the middle value in a data set when it is arranged in ascending or descending order. If there is an even number of values, the median is the average of the two middle values.\n",
    "\n",
    "- Mode: The mode is the value that appears most frequently in a dataset. A dataset can have no mode (if all values are unique) or multiple modes (if multiple values have the same highest frequency). The mode is useful for categorical data and discrete distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b11b377",
   "metadata": {},
   "source": [
    "#### 14. What are the Measures of dispersion?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1999139",
   "metadata": {},
   "source": [
    "Measures of dispersion, also known as measures of variability or spread, indicate how much the values in a dataset deviate from the central tendency. They help in quantifying how far the data points vary from the average value.\n",
    "\n",
    "Some of the common Measures of dispersion are as follows:\n",
    "\n",
    "- Range: The range is the difference between the highest and lowest values in a data set. It gives an idea of how much the data spreads from the minimum to the maximum.\n",
    "\n",
    "- Variance: The variance is the average of the squared deviations of each data point from the mean. It is a measure of how spread out the data is around the mean.\n",
    "Variance(σ2)= ∑(X−μ)**2 / N\n",
    "\n",
    "- Standard Deviation: The standard deviation is the square root of the variance. It is a measure of how spread out the data is around the mean, but it is expressed in the same units as the data itself.\n",
    "\n",
    "- Mean Absolute Deviation (MAD): MAD is the average of the absolute differences between each data point and the mean. Unlike variance, it doesn't involve squaring the differences, making it less sensitive to extreme values. it is less sensitive to outliers than the variance or standard deviation.\n",
    "\n",
    "- Percentiles: Percentiles are statistical values that measure the relative positions of values within a dataset. Which is computed by arranging the dataset in descending order from least to the largest and then dividing it into 100 equal parts. In other words, a percentile tells you what percentage of data points are below or equal to a specific value. Percentiles are often used to understand the distribution of data and to identify values that are above or below a certain threshold within a dataset.\n",
    "\n",
    "- Interquartile Range (IQR): The interquartile range (IQR) is the range of values ranging from the 25th percentile (first quartile) to the 75th percentile (third quartile). It measures the spread of the middle 50% of the data and is less affected by outliers.\n",
    "\n",
    "- Coefficient of Variation (CV): The coefficient of variation (CV) is a measure of relative variability, It is the ratio of the standard deviation to the mean, expressed as a percentage. It's used to compare the relative variability between datasets with different units or scales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a27921",
   "metadata": {},
   "source": [
    "#### 15. What is a probability distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880d9ed3",
   "metadata": {},
   "source": [
    "A probability distribution is a mathematical function that estimates the probability of different possible outcomes or events occurring in a random experiment or process. It is a mathematical representation of random phenomena in terms of sample space and event probability, which helps us understand the relative possibility of each outcome occurring.\n",
    "\n",
    "There are two main types of probability distributions:\n",
    "\n",
    "- Discrete Probability Distribution: In a discrete probability distribution, the random variable can only take on distinct, separate values. Each value is associated with a probability. Examples of discrete probability distributions include the binomial distribution, the Poisson distribution, and the hypergeometric distribution.\n",
    "- Continuous Probability Distribution: In a continuous probability distribution, the random variable can take any value within a certain range. These distributions are described by probability density functions (PDFs). Examples of continuous probability distributions include the normal distribution, the exponential distribution, and the uniform distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2987aa03",
   "metadata": {},
   "source": [
    "#### 16. What are normal distributions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A normal distribution, also known as a Gaussian distribution, is a specific type of probability distribution with a symmetric, bell-shaped curve. The data in a normal distribution clustered around a central value i.e mean, and the majority of the data falls within one standard deviation of the mean. The curve gradually tapers off towards both tails, showing that extreme values are becoming distribution having a mean equal to 0 and standard deviation equal to 1 is known as standard normal distribution and Z-scores are used to measure how many standard deviations a particular data point is from the mean in standard normal distribution.\n",
    "\n",
    "Normal distributions are a fundamental concept that supports many statistical approaches and helps researchers understand the behaviour of data and variables in a variety of scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf84234",
   "metadata": {},
   "source": [
    "#### 17. What is the central limit theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aae6546",
   "metadata": {},
   "source": [
    "The Central Limit Theorem (CLT) is a fundamental concept in statistics that states that, under certain conditions, the distribution of sample means approaches a normal distribution as sample size rises, regardless of the the original population distribution. In other words, even if the population distribution is not normal, when the sample size is high enough, the distribution of sample means will tend to be normal.\n",
    "\n",
    "The Central Limit Theorem has three main assumptions:\n",
    "\n",
    "- The samples must be independent. This means that the outcome of one sample cannot affect the outcome of another sample.\n",
    "- The samples must be random. This means that each sample must be drawn from the population in a way that gives all members of the population an equal chance of being selected.\n",
    "- The sample size must be large enough. The CLT typically applies when the sample size is greater than 30."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f1f71b",
   "metadata": {},
   "source": [
    "#### 18. What are the null hypothesis and alternative hypotheses?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e9e6fc",
   "metadata": {},
   "source": [
    "In statistics, the null and alternate hypotheses are two mutually exclusive statements regarding a population parameter. A hypothesis test analyzes sample data to determine whether to accept or reject the null hypothesis. Both null and alternate hypotheses represent the opposing statements or claims about a population or a phenomenon under investigation.\n",
    "\n",
    "- Null Hypothesis: The null hypothesis is a statement regarding the status quo representing no difference or effect after the phenomena unless there is strong evidence to the contrary.\n",
    "- Alternate Hypothesis: The alternate hypothesis is a statement that disregards the status quo means supports the difference or effect. The researcher tries to prove the hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d58f651",
   "metadata": {},
   "source": [
    "#### 19. What is a p-value, and what does it mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455f6537",
   "metadata": {},
   "source": [
    "A p-value, which stands for \"probability value,\" is a statistical metric used in hypothesis testing to measure the strength of evidence against a null hypothesis. When the null hypothesis is considered to be true, it measures the chance of receiving observed outcomes (or more extreme results). In layman's words, the p-value determines whether the findings of a study or experiment are statistically significant or if they might have happened by chance.\n",
    "\n",
    "The p-value is a number between 0 and 1, which is frequently stated as a decimal or percentage. If the null hypothesis is true, it indicates the probability of observing the data (or more extreme data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45748ed",
   "metadata": {},
   "source": [
    "#### 20. What is the significance level?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee9b1bf",
   "metadata": {},
   "source": [
    "The significance level, often denoted as α (alpha), is a critical parameter in hypothesis testing and statistical analysis. It defines the threshold for determining whether the results of a statistical test are statistically significant. In other words, it sets the standard for deciding when to reject the null hypothesis (H0) in favor of the alternative hypothesis (Ha).\n",
    "\n",
    "If the p-value is less than the significance level, we reject the null hypothesis and conclude that there is a statistically significant difference between the groups.\n",
    "\n",
    "- If p-value ≤ α: Reject the null hypothesis. This indicates that the results are statistically significant, and there is evidence to support the alternative hypothesis.\n",
    "- If p-value > α: Fail to reject the null hypothesis. This means that the results are not statistically significant, and there is insufficient evidence to support the alternative hypothesis.\n",
    "\n",
    "The choice of a significance level involves a trade-off between Type I and Type II errors. A lower significance level (e.g., α = 0.01) decreases the risk of Type I errors while increasing the chance of Type II errors (failure to identify a real impact). A higher significance level (e.g., = 0.10), on the other hand, increases the probability of Type I errors while decreasing the chance of Type II errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa75b6",
   "metadata": {},
   "source": [
    "#### 21. Describe Type I and Type II errors in hypothesis testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57107c2f",
   "metadata": {},
   "source": [
    "In hypothesis testing, When deciding between the null hypothesis (H0) and the alternative hypothesis (Ha), two types of errors may occur. These errors are known as Type I and Type II errors, and they are important considerations in statistical analysis.\n",
    "\n",
    "- Type I error (False Positive, α): Type I error occurs when the null hypothesis is rejected when it is true. This is also referred as a false positive. The probability of committing a Type I error is denoted by α (alpha) and is also known as the significance level. A lower significance level (e.g., = 0.05) reduces the chance of Type I mistakes while increasing the risk of Type II errors.\n",
    "\n",
    "For example, a Type I error would occur if we estimated that a new medicine was successful when it was not.\n",
    "\n",
    "Type I Error (False Positive, α): Rejecting a true null hypothesis.\n",
    "\n",
    "- Type II Error (False Negative, β): Type II error occurs when a researcher fails to reject the null hypothesis when it is actually false. This is also referred as a false negative. The probability of committing a Type II error is denoted by β (beta)\n",
    "\n",
    "For example, a Type II error would occur if we estimated that a new medicine was not effective when it is actually effective.\n",
    "\n",
    "Type II Error (False Negative, β): Failing to reject a false null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fddc1bb",
   "metadata": {},
   "source": [
    "#### 22. What is a confidence interval, and how does it is related to point estimates?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec982d4d",
   "metadata": {},
   "source": [
    "The confidence interval is a statistical concept used to estimates the uncertainty associated with estimating a population parameter (such as a population mean or proportion) from a sample. It is a range of values that is likely to contain the true value of a population parameter along with a level of confidence in that statement.\n",
    "\n",
    "- Point estimate: A point estimate is a single that is used to estimate the population parameter based on a sample. For example, the sample mean (x̄) is a point estimate of the population mean (μ). The point estimate is typically the sample mean or the sample proportion.\n",
    "- Confidence interval: A confidence interval, on the other hand, is a range of values built around a point estimate to account for the uncertainty in the estimate. It is typically expressed as an interval with an associated confidence level (e.g., 95% confidence interval). The degree of confidence or confidence level shows the probability that the interval contains the true population parameter.\n",
    "\n",
    "The relationship between point estimates and confidence intervals can be summarized as follows:\n",
    "\n",
    "- A point estimate provides a single value as the best guess for a population parameter based on sample data.\n",
    "- A confidence interval provides a range of values around the point estimate, indicating the range of likely values for the population parameter.\n",
    "- The confidence level associated with the interval reflects the level of confidence that the true parameter value falls within the interval.\n",
    "- For example, A 95% confidence interval indicates that you are 95% confident that the real population parameter falls inside the interval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad1e117",
   "metadata": {},
   "source": [
    "#### 23. What is ANOVA in Statistics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANOVA, or Analysis of Variance, is a statistical technique used for analyzing and comparing the means of two or more groups or populations to determine whether there are statistically significant differences between them or not. It is a parametric statistical test which means that, it assumes the data is normally distributed and the variances of the groups are identical. It helps researchers in determining the impact of one or more categorical independent variables (factors) on a continuous dependent variable.\n",
    "\n",
    "ANOVA works by partitioning the total variance in the data into two components:\n",
    "\n",
    "- Between-group variance: It analyzes the difference in means between the different groups or treatment levels being compared.\n",
    "- Within-group variance: It analyzes the variance within each individual group or treatment level.\n",
    "\n",
    "Depending on the investigation's design and the number of independent variables, ANOVA has numerous varieties:\n",
    "\n",
    "- One-Way ANOVA: Compares the means of three or more independent groups or levels of a single categorical variable. For Example: One-way ANOVA can be used to compare the average age of employees among the three different teams in a company.\n",
    "- Two-Way ANOVA: Compare the means of two or more independent groups while taking into account the impact of a two independent categorical variables (factors) . For example, Two-way ANOVA can be to compare the average age of employees among the three different teams in a company, while also taking into account the gender of the employees.\n",
    "- Multivariate Analysis of Variance (MANOVA): Compare the means of multiple dependent variables. For example, MANOVA can be used to compare the average age, average salary, and average experience of employees among the three different teams in a company."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ec6ad6",
   "metadata": {},
   "source": [
    "#### 24. What is a correlation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4a5565",
   "metadata": {},
   "source": [
    "Correlation is a statistical term that analyzes the degree of a linear relationship between two or more variables. It estimates how effectively changes in one variable predict or explain changes in another.Correlation is often used to access the strength and direction of associations between variables in various fields, including statistics, economics.\n",
    "\n",
    "The correlation between two variables is represented by correlation coefficient, denoted as \"r\". The value of \"r\" can range between -1 and +1, reflecting the strength of the relationship:\n",
    "\n",
    "- Positive correlation (r > 0): As one variable increases, the other tends to increase. The greater the positive correlation, the closer \"r\" is to +1.\n",
    "- Negative correlation (r < 0): As one variable rises, the other tends to fall. The closer \"r\" is to -1, the greater the negative correlation.\n",
    "- No correlation (r = 0): There is little or no linear relationship between the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdd8a34",
   "metadata": {},
   "source": [
    "#### 25. What are the differences between Z-test, T-test and F-test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Z-test, t-test, and F-test are statistical hypothesis tests that are employed in a variety of contexts and for a variety of objectives.\n",
    "\n",
    "- Z-test: The Z-test is performed when the population standard deviation is known. It is a parametric test, which means that it makes certain assumptions about the data, such as that the data is normally distributed. The Z-test is most accurate when the sample size is large.\n",
    "\n",
    "- T-test: The T-test is performed when the population standard deviation is unknown. It is also a parametric test, but unlike the Z-test, it is less sensitive to violations of the normality assumption. The T-test is most accurate when the sample size is large.\n",
    "\n",
    "- F-test: The F-test is performed to compare two or more groups' variances. It assume that populations being compared follow a normal distribution.. When the sample sizes of the groups are equal, the F-test is most accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 26. What is linear regression ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is a statistical approach that fits a linear equation to observed data to represent the connection between a dependent variable (also known as the target or response variable) and one or more independent variables (also known as predictor variables or features). It is one of the most basic and extensively used regression analysis techniques in statistics and machine learning. Linear regression presupposes that the independent variables and the dependent variable have a linear relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fee17d7",
   "metadata": {},
   "source": [
    "#### 27. What is DBMS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBMS stands for Database Management System. It is software designed to manage, store, retrieve, and organize data in a structured manner. It provides an interface or a tool for performing CRUD operations into a database. It serves as an intermediary between the user and the database, allowing users or applications to interact with the database without the need to understand the underlying complexities of data storage and retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59279812",
   "metadata": {},
   "source": [
    "#### 28. What are the basic SQL CRUD operations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea639c7",
   "metadata": {},
   "source": [
    "SQL CRUD stands for CREATE, READ(SELECT), UPDATE, and DELETE statements in SQL Server. CRUD is nothing but Data Manipulation Language (DML) Statements. CREATE operation is used to insert new data or create new records in a database table, READ operation is used to retrieve data from one or more tables in a database, UPDATE operation is used to modify existing records in a database table and DELETE is used to remove records from the database table based on specified conditions. Following are the basic query syntax examples of each operation:\n",
    "\n",
    "CREATE\n",
    "It is used to create the table and insert the values in the database. The commands used to create the table are as follows:\n",
    "\n",
    "``INSERT INTO employees (first_name, last_name, salary)\n",
    "VALUES ('Pawan', 'Gunjan', 50000);``\n",
    "\n",
    "READ\n",
    "Used to retrive the data from the table\n",
    "\n",
    "``SELECT * FROM employees;``\n",
    "\n",
    "UPDATE\n",
    "Used to modify the existing records in the database table\n",
    "\n",
    "``UPDATE employees\n",
    "SET salary = 55000\n",
    "WHERE last_name = 'Gunjan';``\n",
    "\n",
    "DELETE\n",
    "Used to remove the records from the database table\n",
    "\n",
    "``DELETE FROM employees\n",
    "WHERE first_name = 'Pawan';``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10fb0b2",
   "metadata": {},
   "source": [
    "#### 29. What is the SQL statement used to insert new records into a table?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55a4a20",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f8bc97e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da9ff5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39fb3587",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49fe2db1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
